---
title: "Alireza darvishi-Q14-HW4"
output:
  html_document:
    df_print: paged
---

a
```{r}
set.seed (1)
x1=runif(100)
x2=0.5*x1+rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
```
linear model:

$$y=2+2x_1+0.3x_2+\epsilon$$

b
```{r}
cor(x1,x2)
library(ggplot2)
ggplot(mapping = aes(x=x1,y=x2))+geom_point()
```

c
```{r}
lm_c=lm(formula = y~x1+x2)
summary(lm_c)
```
we can see $\hat{\beta_0}=2.13 ,\ \hat{\beta_1}=1.44 ,\ \hat{\beta_2}=1.01$ are very different from true values mentiond above

we can not reject null hypothesis $H_0: \ \beta_2=0$ with a good confidence but we can do than for $H_0: \ \beta_1=0$ with more than 99.999% confidence

d
```{r}
lm_d=lm(formula = y~x1)
summary(lm_d)
```
yes we can reject null hypothesis $H_0: \ \beta_1=0$  with more than 99.999% confidence but the answer is not what we expected: $2+0.5\times0.3=2.15$

e
```{r}
lm_e=lm(formula = y~x2)
summary(lm_e)
```
yes we can reject null hypothesis $H_0: \ \beta_2=0$  with more than 99.999% confidence but the answer is not what we expected: $0.3+2\times\frac{1}{0.5}=4.3$

f
```{r}

```
There is no contradiction and each variable can be fitted to y separately because:
$$1.9759=1.4396+1.0097\times 0.48766$$
g
```{r}
x1=c(x1, 0.1)
x2=c(x2, 0.8)
y=c(y,6)
par(mfrow=c(2,2))

lm_cg=lm(formula = y~x1+x2)
summary(lm_cg)
plot(lm_cg)

lm_dg=lm(formula = y~x1)
summary(lm_dg)
plot(lm_dg)

lm_eg=lm(formula = y~x2)
summary(lm_eg)
plot(lm_eg)
```
it is an outlier in last model and the last point is a high-leverage point

